{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f71700f-c9bc-4272-a672-8fcfdf5e7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q genanki opencc\n",
    "\n",
    "import os, re, json, glob\n",
    "import genanki\n",
    "import pandas as pd\n",
    "from opencc import OpenCC\n",
    "\n",
    "opencc_tw2s = OpenCC('tw2s')\n",
    "\n",
    "SOURCES = ['slides', 'quizlet', 'ankiweb', 'ddsg', 'ccc', 'mquizlet']\n",
    "DFS = {}\n",
    "\n",
    "for name in SOURCES:\n",
    "    df = pd.read_csv(f'data/{name}.tsv', sep='\\t', index_col=False)\n",
    "    assert sum(df['ID'].isnull()) == 0\n",
    "    assert sum(df['Traditional'].isnull()) == 0\n",
    "    if len(set(df.ID)) != len(df):\n",
    "        print(name, df[df.ID.isin(df.ID.value_counts()[lambda X: X >= 2].index)])\n",
    "    for col in df:\n",
    "        df[col] = df[col].fillna('')\n",
    "    df = df.sort_values('ID').set_index('ID').copy()\n",
    "    DFS[name] = df\n",
    "\n",
    "slides_df = DFS['slides']\n",
    "\n",
    "tags_df = pd.read_csv('data/tags.tsv', sep='\\t', comment='#').set_index('ID').copy()\n",
    "\n",
    "# Diffs reviewed against the book + extra terms from book\n",
    "book_df = pd.read_csv('data/book.tsv', sep='\\t', comment='#')\n",
    "assert len(set(book_df.ID)) == len(book_df), book_df.ID.value_counts().head()\n",
    "book_df = book_df.sort_values('ID').set_index('ID').copy()\n",
    "\n",
    "termid_df = pd.read_csv('data/term-ids.tsv', sep='\\t', comment='#')\n",
    "trad_variants_mp = termid_df.groupby('ID').Traditional.apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523be8fe-859d-4b9f-b5e7-5cfc296aa288",
   "metadata": {},
   "source": [
    "# Merge sources\n",
    "\n",
    "Merges input sources, diffs them for review, generates final outputs: `dangdai.csv`, `dangdai.apkg`, `dangdai-pleco.txt`.\n",
    "\n",
    "Needs `data/*.tsv` from `slides.ipynb` and `extdecks.ipynb` - checked in to repo, don't need to re-run them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea0968-2e24-413a-b465-a0aec6da11e9",
   "metadata": {},
   "source": [
    "## Diff for manual review\n",
    "\n",
    "Generates `diffs.tsv`, review and then pick correct lines into `reviewed.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84801aae-7302-433f-ae84-37fd56d9ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         slides quizlet ankiweb    ddsg     ccc mquizlet \n",
      " slides       0     162     184     226     181     562  0\n",
      "quizlet     162       0     198     284     198     280  162\n",
      "ankiweb     184     198       0     286      35     396  276\n",
      "   ddsg     226     284     286       0     267     501  423\n",
      "    ccc     181     198      35     267       0     399  428\n",
      "mquizlet     562     280     396     501     399       0  858\n",
      "\n",
      "Diffs by book:\n",
      "B1     86\n",
      "B2     70\n",
      "B3    148\n",
      "B4    228\n",
      "B5    184\n",
      "B6    142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def postprocess_chars_for_diff(text):\n",
    "    text = text.replace('＋ ', ' + ')\n",
    "    text = text.replace(' ＋', ' + ')\n",
    "    mp = {\n",
    "        '…': '...',\n",
    "        '‘': \"'\",\n",
    "        '’': \"'\",\n",
    "        '“': '\"',\n",
    "        '”': '\"',\n",
    "        'ﬃ': 'ffi',\n",
    "        'ﬄ': 'ffl',\n",
    "        'ﬁ': 'fi',\n",
    "        'ﬂ': 'fl',\n",
    "        'ﬀ': 'ff',\n",
    "        '／': '/',\n",
    "        '％': '%',\n",
    "        '～': '~',\n",
    "        '＂': '\"',\n",
    "        '＝': '=',\n",
    "        '＋': '+',\n",
    "    }\n",
    "    return ''.join(mp.get(c, c) for c in text)\n",
    "\n",
    "def quick_diff(dfs, verbose=False):\n",
    "    deltas = set()\n",
    "    for col in set(sum([list(d.columns) for d in dfs], start=[])):\n",
    "        if col in ['ID', 'Tags', 'Variants', 'Simplified', 'Audio']: continue\n",
    "        mp = {}\n",
    "        for d in dfs:\n",
    "            for row in d.itertuples():\n",
    "                val = getattr(row, col)\n",
    "                val = val.strip()\n",
    "                if col == 'POS':\n",
    "                    val = ' '.join(sorted(val.replace(',', '/').replace(' ', '').split('/')))\n",
    "                else:\n",
    "                    val = postprocess_chars_for_diff(val)\n",
    "                mp.setdefault(row.Index, []).append(val)\n",
    "        for key, vals in mp.items():\n",
    "            if len(set(vals)) <= 1:\n",
    "                continue\n",
    "            if len(set(vals)) == 2:\n",
    "                a, b = set(vals)\n",
    "                if len(a) > len(b):\n",
    "                    a, b = b, a\n",
    "                if b == a + \"'\" or b == a + \".\":  # ccc deck has many trailing '\n",
    "                    continue\n",
    "            if col == 'POS' and set(vals) == set(['', 'Ph']):\n",
    "                continue\n",
    "            deltas.add(key)\n",
    "            if verbose:\n",
    "                print(key, col, vals)\n",
    "    return list(sorted(deltas))\n",
    "\n",
    "print(''.join('%7s ' % s for s in [''] + SOURCES))\n",
    "dd = []\n",
    "for s1 in SOURCES:\n",
    "    print('%7s ' % s1, end='')\n",
    "    for s2 in SOURCES:\n",
    "        print('%7d ' % len(quick_diff([DFS[s1], DFS[s2]])), end='')\n",
    "    dd.append(DFS[s1])\n",
    "    print(' %d' % len(quick_diff(dd)))\n",
    "\n",
    "delta_ids = quick_diff(list(DFS.values()))\n",
    "\n",
    "print('\\nDiffs by book:\\n%s' % pd.Series(delta_ids).str.slice(0,2).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7609e9-099d-4088-a5b6-7d07186657ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858 -> 0 to review\n"
     ]
    }
   ],
   "source": [
    "#delta_ids_ex_review = list(sorted(set(delta_ids)))\n",
    "delta_ids_ex_review = list(sorted(set(delta_ids) - set(book_df.index)))\n",
    "print('%d -> %d to review' % (len(delta_ids), len(delta_ids_ex_review)))\n",
    "\n",
    "if len(delta_ids_ex_review) > 0:\n",
    "    with open('diffs.tsv', 'w') as fout:\n",
    "        OUT_COLS = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning', 'Tags']\n",
    "        fout.write('\\t'.join(OUT_COLS) + '\\n')\n",
    "        \n",
    "        for term_id in delta_ids_ex_review:\n",
    "            slides_row = slides_df.loc[term_id].to_dict()\n",
    "            merged_rows = []\n",
    "            for src in SOURCES:\n",
    "                if term_id not in DFS[src].index: continue\n",
    "                row = DFS[src].loc[term_id].to_dict()\n",
    "                merged = dict(slides_row)\n",
    "                merged['ID'] = term_id\n",
    "                merged['Tags'] = ''\n",
    "                for col in row:\n",
    "                    if col not in merged: continue\n",
    "                    merged[col] = row[col]\n",
    "\n",
    "                str1 = '\\t'.join([merged[c] for c in OUT_COLS if c != 'Tags'])\n",
    "                match = -1\n",
    "                for i, merged2 in enumerate(merged_rows):\n",
    "                    str2 = '\\t'.join([merged2[c] for c in OUT_COLS if c != 'Tags'])\n",
    "                    if str1 == str2:\n",
    "                        match = i\n",
    "                        break\n",
    "\n",
    "                if match >= 0:\n",
    "                    merged_rows[match]['Tags'] = (merged_rows[match]['Tags'] + ' %s' % src).strip()\n",
    "                else:\n",
    "                    merged['Tags'] = (merged['Tags'] + ' %s' % src).strip()\n",
    "                    merged_rows.append(merged)\n",
    "\n",
    "            for merged in merged_rows:\n",
    "                fout.write('#%s\\n' % '\\t'.join([merged[c] for c in OUT_COLS]))\n",
    "\n",
    "    print('Created diffs.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315e1b4-4055-4f8b-9297-dd1be56d7253",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7648fb33-b3d5-4e17-8226-d04d5b3d04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(delta_ids_ex_review) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960acae4-0aeb-464b-9764-ac32f28cac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "B1    569\n",
       "B2    659\n",
       "B3    851\n",
       "B4    997\n",
       "B5    927\n",
       "B6    957\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term count by book\n",
    "# Reference: http://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/181220.pdf\n",
    "# 569 659 851 997 926(miscounted?) 957 total 4959\n",
    "term_ids = set(book_df.index)\n",
    "for df in DFS.values():\n",
    "    term_ids |= set(df.index)\n",
    "term_ids = list(sorted(set(term_ids)))\n",
    "print(len(term_ids))\n",
    "pd.Series(term_ids).str.slice(0, 2).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448da160-3041-442c-827f-b614ba1f4d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4960\n"
     ]
    }
   ],
   "source": [
    "ankiweb_df = DFS['ankiweb']\n",
    "ankiweb_pinyin_to_audio = ankiweb_df.set_index('Pinyin').Audio.to_dict()\n",
    "\n",
    "slides_df = DFS['slides']\n",
    "\n",
    "# Final corrections + variant expansions\n",
    "errata_df = pd.read_csv('data/errata.tsv', sep='\\t', comment='#').fillna('').set_index(['ID', 'Column'])\n",
    "variants_mp = json.loads(open('data/variants.json').read())\n",
    "\n",
    "rows = []\n",
    "for term_id in term_ids:\n",
    "    row = None\n",
    "    if term_id in book_df.index:\n",
    "        row = book_df.loc[term_id].to_dict()\n",
    "    elif term_id in slides_df.index:\n",
    "        row = slides_df.loc[term_id].to_dict()\n",
    "    else:\n",
    "        row = DFS['mquizlet'].loc[term_id].to_dict()\n",
    "\n",
    "    row = dict(row)\n",
    "    row['ID'] = term_id\n",
    "    for key in row:\n",
    "        if row[key] != row[key]:\n",
    "            row[key] = ''\n",
    "\n",
    "    tags = row.get('Tags', '')\n",
    "    if term_id in tags_df.index:\n",
    "        assert tags_df.loc[term_id, 'Traditional'] in trad_variants_mp[term_id], (row, tags_df.loc[term_id, 'Traditional'], trad_variants_mp[term_id])\n",
    "        tags += ' ' + tags_df.loc[term_id, 'Tags']\n",
    "    tags = list(sorted(set(tags.split())))\n",
    "    if 'Ph' in tags:\n",
    "        assert term_id[1] in '1234'\n",
    "        assert row['POS'] == '', row\n",
    "        row['POS'] = 'Ph'\n",
    "\n",
    "    if 'Simplified' not in row or not row['Simplified']:\n",
    "        row['Simplified'] = opencc_tw2s.convert(row['Traditional'])\n",
    "\n",
    "    tags = list(sorted(set(tags)))\n",
    "    tags = [t for t in tags if t not in DFS.keys()]\n",
    "    tags = [t for t in tags if not re.match('^(Ph|Place|book|edit-.*|wikt|simp|sic|Flagged.*)$', t)]\n",
    "    row['Tags'] = ' '.join(tags)\n",
    "\n",
    "    # generate with audio.ipynb or set to '' to disable audio\n",
    "    assert os.path.exists(f'data/media/dangdai-{term_id}.mp3')\n",
    "    row['Audio'] = f'[sound:dangdai-{term_id}.mp3]'\n",
    "\n",
    "    for col in row:\n",
    "        key = row['ID'], col\n",
    "        if key in errata_df.index:\n",
    "            #print(key, row[col], errata_df.loc[key].to_dict())\n",
    "            assert row[col] == errata_df.loc[key]['Original'], (col, row[col], errata_df.loc[key])\n",
    "            row[col] = errata_df.loc[key]['Corrected']\n",
    "            assert row[col] == row[col].strip()\n",
    "\n",
    "    row['Variants'] = ''\n",
    "    if term_id in variants_mp:\n",
    "        assert row['Traditional'] == variants_mp[term_id]['Traditional']\n",
    "        assert row['Pinyin'] == variants_mp[term_id]['Pinyin']\n",
    "        row['Variants'] = json.dumps(variants_mp[term_id]['Variants'], ensure_ascii=False)\n",
    "\n",
    "    assert row['Traditional']\n",
    "    assert row['Simplified']\n",
    "    assert row['Pinyin']\n",
    "    assert row['POS'] or 'Name' in tags\n",
    "    assert row['Meaning']\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning', 'Audio', 'Variants', 'Tags']].set_index('ID').fillna('').copy()\n",
    "df.to_csv('dangdai.csv', index=True)\n",
    "\n",
    "dangdai_df = df\n",
    "print(len(dangdai_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7be56-c0f9-4c74-bac0-cd2ef4add52c",
   "metadata": {},
   "source": [
    "## Expand variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead049f7-c122-449d-9252-2cdd8f07153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dangdai-expanded.csv: 4989 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expanded_rows = []\n",
    "for row in pd.read_csv('dangdai.csv', dtype='str').fillna('').to_dict(orient='records'):\n",
    "    for var_dict in json.loads(row['Variants'] or '[{}]'):\n",
    "        var = dict(row)\n",
    "        var.update(var_dict)\n",
    "        var.pop('Variants')\n",
    "        assert '/' not in var['Simplified']\n",
    "        expanded_rows.append(var)\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df.to_csv('dangdai-expanded.csv', index=False)\n",
    "print('dangdai-expanded.csv: %d rows\\n' % len(expanded_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b5ccf-625f-40be-8843-a88c87fb02a6",
   "metadata": {},
   "source": [
    "## Export in pleco format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ee7aa2-3e44-4d3f-a6b8-c6c09ebb75b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 384748 Nov  8 18:09 dangdai-pleco.txt\n"
     ]
    }
   ],
   "source": [
    "EAC1_TAG = '\\uEAC1\\uEC00\\uEC00\\uECCC\\uEC99'  # lesson tag color, #00cc99 green\n",
    "\n",
    "df = pd.read_csv('dangdai.csv', dtype='str').set_index('ID').fillna('').copy()\n",
    "\n",
    "# Clean up a few variants that don't display nicely, otherwise pleco can cope with variants here fine\n",
    "df.loc['B1L01-I-18', 'Traditional'] = '臺/台灣'  # / separates char variants in pleco\n",
    "df.loc['B2L02-I-18', 'Pinyin'] = 'Táiběi Yīlíngyī'\n",
    "df.loc['B3L08-II-46', 'Traditional'] = '高級管理人員工商管理碩士'\n",
    "df.loc['B3L08-II-46', 'Simplified'] = '高级管理人员工商管理硕士'\n",
    "df.loc['B3L08-II-46', 'Pinyin'] = 'gāojí guǎnlǐ rényuán gōngshāng guǎnlǐ shuòshì'\n",
    "\n",
    "with open('dangdai-pleco.txt', 'w') as fout:\n",
    "    last_header = ''\n",
    "    for row in df.itertuples():\n",
    "        m = re.match('^B([1-6])L([0-9]{2})-(I+)-.*', row.Index)\n",
    "        header = f'//當代中文/Book {m[1]}/L{m[2]}-{m[3]}'\n",
    "        if header != last_header:\n",
    "            fout.write(header + '\\n')\n",
    "            last_header = header\n",
    "\n",
    "        defn = f'({row.POS}) {row.Meaning}' if row.POS else row.Meaning\n",
    "        defn += f' {EAC1_TAG}[D{m[1]}L{int(m[2])}]\\uEAC2'  # [DnLn] lesson tag in color\n",
    "        fout.write(f'{row.Simplified}[{row.Traditional}]\\t{row.Pinyin}\\t{defn}\\n')\n",
    "\n",
    "!ls -l dangdai-pleco.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac546d25-3a25-4d7b-bade-ca48479b25a9",
   "metadata": {},
   "source": [
    "## Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c72bdfb-918c-4e9f-a86e-98b311606db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dangdai.csv', dtype='str').set_index('ID').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b00c21c-7565-41f1-87aa-128be0f0ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../unihan/unihan.csv'):\n",
    "    unihan_df = pd.read_csv('../unihan/unihan.csv', dtype='str').fillna('').set_index('char')\n",
    "    for row in df.itertuples():\n",
    "        assert re.match(r'^([\\u4e00-\\u9fff（）＝！、，．…1012/]|-$| [+] |X | Y| X |KTV|BBC|EMBA|SSL|LED|number|noun)+$', row.Traditional), row.Traditional\n",
    "        for c in row.Traditional:\n",
    "            if c in '（）＝，、．！…-/KTVBBCEMBAXYSSLLED +number noun 101 2': continue\n",
    "            sc = unihan_df.kSimplifiedVariant[c]\n",
    "            tc = unihan_df.kTraditionalVariant[c]\n",
    "            assert tc == '' or (sc != '' and tc != ''), (row.Traditional, c, unihan_df.kTraditionalVariant[c])\n",
    "\n",
    "        for var_dict in json.loads(row.Variants or '[{}]'):\n",
    "            var = dict(row._asdict())\n",
    "            var.update(var_dict)\n",
    "            assert re.match(r'^([\\u4e00-\\u9fff…！、，．1012]|-$|KTV|BBC| X )+$', var['Traditional']), var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963b3831-59b6-4ddb-bab5-8e20f98c24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KTV', 'KTV', 'KTV', 'N'] vs ['']\n",
      "['台北101', '台北101', 'Táiběi 101', ''] vs ['tāiběiyīlíngyī', 'tāi běiyīlíngyī', 'tāiběi yīlíngyī', 'tāi běi yīlíngyī', 'tāiběiyī língyī', 'tāi běiyī língyī', 'tāiběi yī língyī', 'tāi běi yī língyī', 'tāiběiyīlíng yī', 'tāi běiyīlíng yī']\n",
      "['古蹟', '古迹', 'gǔjī', 'N'] vs ['gǔjì', 'gǔ jì', 'gǔjì ', 'gǔ jì ']\n",
      "['阿嬤', '阿嬷', 'āmà', 'N'] vs ['āmo', 'ā mo', 'āmo ', 'ā mo ', 'āmó', 'ā mó', 'āmó ', 'ā mó ', 'āmā', 'ā mā']\n",
      "['照 X 光', '照 X 光', 'zhào X guāng', 'Ph'] vs ['zhàoguāng', 'zhào guāng', 'zhàoguāng ', 'zhào guāng ']\n",
      "['就…而言', '就…而言', 'jiù...éryán', 'Ph'] vs [\"jiù'éryán\", 'jiù éryán', \"jiù'ér yán\", 'jiù ér yán', \"jiù'éryán \", 'jiù éryán ', \"jiù'ér yán \", 'jiù ér yán ', \"jiu'éryán\", 'jiu éryán']\n",
      "['阿喀郎．汗', '阿喀郎．汗', 'Ākèláng Hàn', ''] vs ['ākālànghàn', 'ā kālànghàn', 'ākā lànghàn', 'ā kā lànghàn', 'ākālàng hàn', 'ā kālàng hàn', 'ākā làng hàn', 'ā kā làng hàn', 'ākālànghàn ', 'ā kālànghàn ']\n",
      "['BBC', 'BBC', 'BBC', ''] vs ['bībī', 'bī bī', 'bībī ', 'bī bī ']\n"
     ]
    }
   ],
   "source": [
    "# Readings check against possible syllable readings in cedict\n",
    "if os.path.exists('../cedict/syllables.csv'):\n",
    "    readings_mp = {'一':{'yì','yí'}}\n",
    "    for row in pd.read_csv('../cedict/syllables.csv', dtype='str').fillna('').itertuples():\n",
    "        readings_mp.setdefault(row.Traditional, set()).add(row.Pinyin.lower())\n",
    "\n",
    "    def gen_readings(trad):\n",
    "        if trad == '':\n",
    "            yield ''\n",
    "        elif trad[0] not in readings_mp:\n",
    "            yield from gen_readings(trad[1:])\n",
    "        else:\n",
    "            for x in readings_mp[trad[0]]:\n",
    "                for y in gen_readings(trad[1:]):\n",
    "                    yield x.lower() + (\"'\" if y and y[0] in 'aāáǎàeēéěèoōóǒò' else '') + y\n",
    "                    yield x.lower() + ' ' + y\n",
    "\n",
    "    for row in df.to_dict(orient='records'):\n",
    "        for var_dict in json.loads(row['Variants'] or '[{}]'):\n",
    "            var = dict(row)\n",
    "            var.update(var_dict)\n",
    "            readings = list(gen_readings(var['Traditional']))\n",
    "            if re.sub(\"([-,.?!]|/.*)\", '', var['Pinyin']).lower() not in readings:\n",
    "                print(list(var.values())[:4], 'vs', readings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccebfc9e-54a6-411a-8184-4e7735115203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slides 247   quizlet 147   ankiweb 112   ddsg 324   ccc 131   mquizlet 470   "
     ]
    }
   ],
   "source": [
    "assert list(sorted(set(df.index))) == term_ids\n",
    "assert len(set(df.index)) == len(df)\n",
    "\n",
    "# Diff combined to sources\n",
    "for src in SOURCES:\n",
    "    print('%s %d' % (src, len(quick_diff([df, DFS[src]]))), end='   ')\n",
    "\n",
    "#d = df.reset_index()[['ID', 'Traditional', 'Pinyin', 'POS', 'Meaning']].sort_values('ID').copy()\n",
    "#d.loc[d.ID.str.match('B[1-4].*') & d.POS == 'Ph', 'POS'] = ''\n",
    "#d.to_csv('z-dangdai.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5f9004-d42a-45aa-ab4e-af49bdce3bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1L01-I-18      臺灣（＝台灣）                \tsimp 台湾                \topencc 台湾（＝台湾）           \tslides 台湾（＝台灣）\n",
      "B3L05-II-26     想像                     \tsimp 想象                \topencc 想像                \tslides 想象\n",
      "B4L04-II-37     邱吉爾                    \tsimp 丘吉尔               \topencc 邱吉尔               \tslides 丘吉尔\n",
      "B4L12-II-23     骯髒                     \tsimp 骯脏                \topencc 肮脏                \tslides 骯脏\n",
      "B5L04-II-33     巴塞隆納                   \tsimp 巴塞隆那              \topencc 巴塞隆纳              \tslides 巴塞隆那\n",
      "B5L08-I-44      索馬利亞                   \tsimp 索马利亞              \topencc 索马利亚              \tslides 索马利亞\n",
      "B5L10-I-18      藉由                     \tsimp 藉由                \topencc 借由                \tslides 藉由\n",
      "B6L02-II-32     發光二極體（LED）             \tsimp 发光二极管（LED）        \topencc 发光二极体（LED）        \tslides 发光二极管（LED）\n",
      "B6L03-I-T-05    瑪莎．葛蘭姆                 \tsimp 马莎．葛兰姆            \topencc 玛莎．葛兰姆            \tslides 马莎．葛兰姆\n",
      "B6L03-I-T-10    溫蒂．威倫                  \tsimp 温迪．威伦             \topencc 温蒂．威伦             \tslides 温迪．威伦\n",
      "B6L04-II-T-02   艾森豪                    \tsimp 艾森豪威尔             \topencc 艾森豪               \tslides 艾森豪威尔\n",
      "B6L06-I-10      數位化                    \tsimp 数字化               \topencc 数位化               \tslides 数字化\n",
      "B6L06-I-24      藉此                     \tsimp 藉此                \topencc 借此                \tslides 藉此\n",
      "B6L06-II-T-01   尼克森                    \tsimp 尼克松               \topencc 尼克森               \tslides 尼克松\n",
      "B6L07-I-37      知識份子                   \tsimp 知识分子              \topencc 知识份子              \tslides 知识分子\n",
      "B6L08-II-T-01   奧會＝奧林匹克委員會             \tsimp 奥委会＝奥林匹克委员会       \topencc 奥会＝奥林匹克委员会        \tslides 奥委会＝奥林匹克委员会\n"
     ]
    }
   ],
   "source": [
    "# Check traditional/simplified match with opencc.\n",
    "# Most are just variants, or sometimes more common PRC terms going a step more than char-by-char replacement\n",
    "slides_df = DFS['slides']\n",
    "for term_id in term_ids:\n",
    "    trad = df.loc[term_id, 'Traditional']\n",
    "    simp = df.loc[term_id, 'Simplified']\n",
    "    simp_sl = slides_df.loc[term_id, 'Simplified'] if term_id in slides_df.index else ''\n",
    "    simp_cc = opencc_tw2s.convert(trad)\n",
    "    if simp != simp_cc:\n",
    "        print('%-15s %-23s\\tsimp %-18s\\topencc %-18s\\tslides %s' % (term_id, trad, simp, simp_cc, simp_sl))\n",
    "\n",
    "# TODO: add SimplifiedAlt column for non-direct translations, 妳, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d376b0e-e48a-4870-8d4b-45c8da8f276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEANING_HZS = '三之了二人以仿作來保倒光兒入全冒出制力化占史名品員啃啊四因國坦基大夫女姓婦子存學定室家寶少師年廢廳彈從心必怖性恐情愛感房改數族日時會板業水河泰清演潔炸熊燈爺片物獄率班生產白的盆監科第節籍糧紅級群習老者聚自至補製說貓身農迷造部都醉量銀長間闖限除隊需題風食髮麻黑'\n",
    "TONES = 'āáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜĀÁǍÀĒÉĚÈĪÍǏÌŌÓǑÒŪÚǓÙÜǗǙǛ'\n",
    "\n",
    "for row in df.itertuples():\n",
    "    assert row.Traditional == row.Traditional.strip()\n",
    "    assert row.Simplified == row.Simplified.strip()\n",
    "\n",
    "    assert row.Meaning == row.Meaning.strip()\n",
    "    for c in row.Meaning:\n",
    "        if re.match('[- a-zA-Z(),.:;=&+%?~!\\'\"/0-9－]', c.lower()): continue\n",
    "        if c in TONES + MEANING_HZS: continue\n",
    "        print('[%c] %s %s' % (c, row.Index, row.Meaning))\n",
    "\n",
    "    assert row.Pinyin == row.Pinyin.strip()\n",
    "    for c in row.Pinyin:\n",
    "        if re.match(\"[- a-z/'()01.,=]\", c.lower()): continue\n",
    "        if c in TONES: continue\n",
    "        print('[%c] %s %s' % (c, row.Index, row.Pinyin))\n",
    "\n",
    "    assert row.POS != '' or 'Name' in row.Tags\n",
    "    assert row.POS == row.POS.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c78e638e-78c6-455b-84fb-cb3ad4b46a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS\n",
       "N              1793\n",
       "V               614\n",
       "Ph              505\n",
       "Vs              431\n",
       "Adv             291\n",
       "                230\n",
       "Vi              161\n",
       "Vst             140\n",
       "V-sep           101\n",
       "Vp               99\n",
       "Id               98\n",
       "M                76\n",
       "Vs-attr          62\n",
       "Ptc              56\n",
       "Conj             56\n",
       "Vpt              50\n",
       "V/N              35\n",
       "Prep             35\n",
       "Vaux             30\n",
       "N/V              25\n",
       "Vp-sep           23\n",
       "Det              21\n",
       "Vs-sep            9\n",
       "Vs-pred           4\n",
       "Vs/N              4\n",
       "Vi/N              3\n",
       "N/Vi              3\n",
       "Vi/V              1\n",
       "Adv/N             1\n",
       "N/Vst             1\n",
       "Vst/N             1\n",
       "Vs-attr/Adv       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.POS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a6489da-cb97-4433-8c3b-fd2c2990aed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tags\n",
       "                  4708\n",
       "Name               223\n",
       "Character Name      29\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Tags.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d977a43a-495a-456e-b408-7ef03080b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(df[df.Variants.fillna('') == ''].Traditional.str.match('^[^/()（）]+$'))\n",
    "assert all(df[df.Variants.fillna('') == ''].Pinyin.str.match('^[^/()（）]+$'))\n",
    "\n",
    "# Generally don't want spaces in hanzi but book already has some ' + number' etc which need them so a few exceptions\n",
    "#df[df.Traditional.str.contains('[ ]')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709c280-dfad-486f-b801-23d183bc32f2",
   "metadata": {},
   "source": [
    "## Generate anki package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa18f346-c2a1-487a-bf45-621bfbe4f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/media/_MoeStandardKai.ttf: OK\n"
     ]
    }
   ],
   "source": [
    "%%bash -e\n",
    "if ! [[ -f data/media/_MoeStandardKai.ttf ]]; then\n",
    "  curl -o data/media/_MoeStandardKai.ttf https://www.moedict.tw/fonts/truetype/moe/MoeStandardKai.ttf\n",
    "  # alternatively get from debian/ubuntu repos https://packages.ubuntu.com/source/mantic/moe-standard-fonts\n",
    "fi\n",
    "echo '6744c2ffd6c011f3e6ceb93f15f9b324699be41c7e47be86eb40593cf1ee8078  data/media/_MoeStandardKai.ttf' | sha256sum -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1119bc34-bc49-47dc-bad2-a4580a419063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 136471276 Nov  8 18:10 dangdai.apkg\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dangdai.csv', dtype='str').set_index('ID').fillna('')\n",
    "\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning', 'Audio', 'Variants']\n",
    "\n",
    "model = genanki.Model(\n",
    "    1696395923,\n",
    "    'Dangdai',\n",
    "    fields=[{'name': c} for c in cols],\n",
    "    templates=[{\n",
    "        'name': 'Dangdai',\n",
    "        'qfmt': open('dangdai-qfmt.html').read(),\n",
    "        'afmt': open('dangdai-afmt.html').read(),\n",
    "    }],\n",
    "    css=open('dangdai.css').read(),\n",
    ")\n",
    "\n",
    "deck = genanki.Deck(\n",
    "    1696395265,\n",
    "    name='dangdai',\n",
    "    description='A Course in Contemporary Chinese (當代中文課程 B1-B6, Traditional)'\n",
    ")\n",
    "\n",
    "for row in df.reset_index().to_dict(orient='records'):\n",
    "    note = genanki.Note(\n",
    "        model=model,\n",
    "        fields=[row[c] for c in cols],\n",
    "        tags=row['Tags'].split(),\n",
    "        guid=genanki.guid_for('dangdai', row['ID'])\n",
    "    )\n",
    "    deck.add_note(note)\n",
    "\n",
    "!rm -f dangdai.apkg\n",
    "package = genanki.Package(deck, media_files=glob.glob('data/media/*'))\n",
    "package.write_to_file('dangdai.apkg')\n",
    "!ls -l dangdai.apkg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10496f3c-e63f-4e7b-b605-0a840fb960ac",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c3c7344-1fe9-4f6c-8cd9-9740a0da63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf downloads/slides/unpacked/\n",
    "!rm -rf downloads/audio/unpacked/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
