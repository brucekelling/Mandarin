{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad9c035-5807-451b-b512-a8b30f3afe23",
   "metadata": {},
   "source": [
    "# Parse vocabulary slides\n",
    "\n",
    "* Downloads and parses vocabulary from slides from the official website: http://mtc.ntnu.edu.tw/chinese-resource.htm\n",
    "* This is the main source of data.\n",
    "* Both traditional and simplified vocab slides parsed.\n",
    "* Corrections for errors in the slides to get usable somewhat cleanish/mergeable data, consistent typography\n",
    "* Output: `data/slides.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892da390-4505-4813-9f3c-81adf89c58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8ab294ed7f9db3ae2f81bf8d70877993af3d65a639a1c066641a431f2c25baa7  DangDai_B1_e.g._of_Grammar__vocabulary_simplified.zip\n",
      "e5e8f41776350ff8e7adb089a9bad6a8ccf3530d0adbd884fba23b6f91395937  DangDai_B1_e.g._of_Grammar__vocabulary_traditional.zip\n",
      "fa1ea0289ce72b764aded52b6dd94b49b5fa5397ff4b8bd7346930b40c09539b  DangDai_B2_e.g._of_Grammar__vocabulary_simplified.zip\n",
      "551bbeb7185bdceb47643de10673558de21c70faf1af9dd7b05a022b294d2b75  DangDai_B2_e.g._of_Grammar__vocabulary_traditional.zip\n",
      "343bed50e89a22bcf2b82dfdd353a4e5d66109b0b9855a1db1fcb5ed17349449  DangDai_B3_e.g._of_Grammar__vocabulary_simplified.zip\n",
      "80af9536c24e367a0116933d0d1ffb64386ff9acd0d121efbc434c7ffa44d873  DangDai_B3_e.g._of_Grammar__vocabulary_traditional.zip\n",
      "722951aca10fb88ee37f462332e0e1a9f0c5861fcfaa649dd9e868ca1447b09a  DangDai_B4_e.g._of_Grammar__vocabulary_simplified.zip\n",
      "81e7cb7e2bea95a05eec48d47ca5bf65af738af0ff083691107e68f16d5479e2  DangDai_B4_e.g._of_Grammar__vocabulary_traditional.zip\n",
      "0cac9f6f0ef4b7e082c173e156ff13cbc239d987fe55a4e8fc65b22b7dd45f50  DangDai_B5_e.g._of_Grammar__vocabulary_simplified.zip\n",
      "7a2d43ff41d90613a626c7148b27e6f2655c96872f256c4f845524a42759502f  DangDai_B5_e.g._of_Grammar__vocabulary_traditional.zip\n",
      "30982a1a6df6cc056e5c73245063b96c0ce884669fcede0b73ee87d9c5209609  DangDai B6  e.g. of Grammar & vocabulary  (simplifiedl).zip\n",
      "7d1cb3d30275efe557f72b7e3cff80a1a530844dab5715532c724cde8a64617e  DangDai B6  e.g. of Grammar & vocabulary  (traditional).zip\n"
     ]
    }
   ],
   "source": [
    "%%bash -e\n",
    "if ! [[ -d downloads/ ]]; then\n",
    "  if [[ -d ../downloads/dangdai ]]; then ln -s ../downloads/dangdai downloads; else mkdir -p downloads; fi\n",
    "fi\n",
    "mkdir -p downloads/slides\n",
    "cd downloads/slides\n",
    "if ! [[ -f 'DangDai B6  e.g. of Grammar & vocabulary  (simplifiedl).zip' ]]; then\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B1_e.g._of_Grammar__vocabulary_traditional.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B1_e.g._of_Grammar__vocabulary_simplified.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B2_e.g._of_Grammar__vocabulary_traditional.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B2_e.g._of_Grammar__vocabulary_simplified.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B3_e.g._of_Grammar__vocabulary_traditional.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B3_e.g._of_Grammar__vocabulary_simplified.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B4_e.g._of_Grammar__vocabulary_traditional.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B4_e.g._of_Grammar__vocabulary_simplified.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B5_e.g._of_Grammar__vocabulary_traditional.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai_B5_e.g._of_Grammar__vocabulary_simplified.zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai%20B6%20%20e.g.%20of%20Grammar%20&%20vocabulary%20%20(traditional).zip'\n",
    "    wget -nc 'https://mtc.ntnu.edu.tw/upload_files/resource/download/Contemporary-Chinese/DangDai%20B6%20%20e.g.%20of%20Grammar%20&%20vocabulary%20%20(simplifiedl).zip'\n",
    "fi\n",
    "sha256sum *.zip\n",
    "\n",
    "mkdir -p unpacked\n",
    "cd unpacked\n",
    "for f in ../DangDai_B*.zip; do rar -o- x \"$f\" >/dev/null; done  # actually .rar\n",
    "for f in ../DangDai*B6*.zip; do 7z -aos x \"$f\" >/dev/null; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7efa796-7012-4bd4-afcd-0573386a3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-pptx opencc genanki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b36cdd8-a362-4598-abea-b291466939d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trad\n",
      "simp\n",
      "B1: 569\n",
      "B2: 659\n",
      "B3: 851\n",
      "B4: 997\n",
      "B5: 927\n",
      "B6: 924\n",
      "Total: 4927\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, os.path\n",
    "import re\n",
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "from opencc import OpenCC\n",
    "\n",
    "opencc_tw2s = OpenCC('tw2s')\n",
    "\n",
    "def pptx_glob(cset):\n",
    "    assert cset in ['trad', 'simp']\n",
    "    res = list(sorted(\n",
    "        glob.glob(f'downloads/slides/unpacked/*{cset}*/*Vocab*/B?-L??.pptx'),\n",
    "        key=os.path.basename\n",
    "    ))\n",
    "    assert len(res) == 15+15+12+12+10+10\n",
    "    return res\n",
    "\n",
    "def extract_para(filepath):  # => list of lists of text paragraphs for each slide\n",
    "    res = []\n",
    "    prs = Presentation(filepath)\n",
    "    k = 0\n",
    "    for slide in prs.slides:\n",
    "        paras = []\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                text = ''.join(run.text for run in paragraph.runs)\n",
    "                text = text.replace('\\t', ' ').strip()\n",
    "                if text:\n",
    "                    paras.append((shape.text_frame.margin_top, shape.text_frame.margin_left, k, text))\n",
    "                    k += 1\n",
    "        paras.sort()\n",
    "        res.append([p[-1] for p in paras])\n",
    "    return res\n",
    "\n",
    "def preprocess_para(para):\n",
    "    res = []\n",
    "    for s in para:\n",
    "        if s in ['風氣fēngqì','风气fēngqì']:\n",
    "            res.extend([s[:2], s[2:]])\n",
    "        elif re.match(r'^[\\u4e00-\\u9fff…一]{4}[a-zA-Z].*[ …][a-zA-Z].*', s):\n",
    "            res.extend([s[:4], s[4:]])\n",
    "        else:\n",
    "            res.append(s)\n",
    "    return res\n",
    "\n",
    "def extract_hanzi(term_id, para):  # -> hanzi, rest of para\n",
    "    HANZI_EXC = set([\n",
    "        '照 X 光',\n",
    "        'X 分之 Y',\n",
    "        'EMBA（高级管理人员 工商管理硕士）',\n",
    "        'EMBA（高級管理人員 工商管理碩士）',\n",
    "        '固态光源（SSL）',\n",
    "        '固態光源（SSL）',\n",
    "        '發光二極體（LED）',\n",
    "        '发光二极管（LED）',\n",
    "    ])\n",
    "\n",
    "    def is_chinese(text):\n",
    "        if text in HANZI_EXC: return True\n",
    "        text = re.sub('([\\t0-9 （）(  ) …、/‧]||＋ number|＋ noun|number ＋|-$)', '', text)\n",
    "        return len(text) >= 1 and all(ord(c) >= 0x4E00 for c in text)\n",
    "\n",
    "    hanzi = None\n",
    "    if term_id == 'B1L07-I-02':\n",
    "        hanzi = 'KTV'\n",
    "    elif term_id == 'B6L06-I-T-02' and 'BBC' in para:\n",
    "        hanzi = 'BBC'\n",
    "    else:\n",
    "        cn_words = [w for w in para if is_chinese(w)]\n",
    "        assert len(cn_words) == 1, (term_id, para, cn_words)\n",
    "        hanzi = cn_words[0]\n",
    "\n",
    "    i = para.index(hanzi)\n",
    "    assert i >= 0\n",
    "    para = para[:i] + para[(i+1):]\n",
    "\n",
    "    hanzi = postprocess_chars(hanzi, True)\n",
    "\n",
    "    if term_id == 'B6L03-I-03':  # incorrect hanzi in slide\n",
    "        hanzi = '舞者'\n",
    "\n",
    "    return hanzi, para\n",
    "\n",
    "def postprocess_chars(text, hanzi):\n",
    "    text = text.replace('＋ ', ' + ')\n",
    "    text = text.replace(' ＋', ' + ')\n",
    "    mp = {\n",
    "        '‘': \"'\",\n",
    "        '’': \"'\",\n",
    "        '“': '\"',\n",
    "        '”': '\"',\n",
    "        'ﬃ': 'ffi',\n",
    "        'ﬄ': 'ffl',\n",
    "        'ﬁ': 'fi',\n",
    "        'ﬂ': 'fl',\n",
    "        'ﬀ': 'ff',\n",
    "        '／': '/',\n",
    "    }\n",
    "    # generally don't want double width characters in meaning field - could lead to font problems\n",
    "    if not hanzi:\n",
    "        mp.update({\n",
    "            '…': '...',\n",
    "            '％': '%',\n",
    "            '～': '~',\n",
    "            '＂': '\"',\n",
    "            '＝': '=',\n",
    "            '＋': '+',\n",
    "        })\n",
    "    text = ''.join(mp.get(c, c) for c in text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def parse_para(para, book_lesson, simplified):\n",
    "    para = preprocess_para(para)\n",
    "\n",
    "    term_id = [s for s in para if re.match('^(I|II)-.*[0-9]+', s)]\n",
    "    assert len(term_id) == 1, para\n",
    "    term_id = term_id[0]\n",
    "    para = [s for s in para if s != term_id]\n",
    "    term_id = term_id.replace(' ', '')\n",
    "    # mangle term id to ascii and make it ascii sortable\n",
    "    term_id = term_id.replace('引-', '0-') # yinyan foreword (B5, before main terms)\n",
    "    term_id = term_id.replace('摘-', '0-') # zhaiyao summary (B6, before main terms)\n",
    "    term_id = term_id.replace('專-', 'T-') # zhuanyoumingci technical terms (B6, after main terms)\n",
    "    term_id = term_id.replace('专-', 'T-')\n",
    "    term_id = book_lesson + '-' + term_id\n",
    "    for n in range(10):\n",
    "        if term_id.endswith('-%d' % n):\n",
    "            term_id = term_id[:-2] + '-0%d' % n\n",
    "            break\n",
    "\n",
    "    term_id_mp = {\n",
    "        # misindexing in slides\n",
    "        'B1L14-I-19': 'B1L14-II-01',\n",
    "        'B1L14-I-20': 'B1L14-II-02',\n",
    "        'B1L14-I-21': 'B1L14-II-03',\n",
    "        'B1L14-I-22': 'B1L14-II-04',\n",
    "        'B1L14-I-23': 'B1L14-II-05',\n",
    "        'B1L14-II-01': 'B1L14-II-06',\n",
    "        'B1L14-II-02': 'B1L14-II-07',\n",
    "        'B1L14-II-03': 'B1L14-II-08',\n",
    "        'B1L14-II-04': 'B1L14-II-09',\n",
    "        'B1L14-II-05': 'B1L14-II-10',\n",
    "        'B1L14-II-06': 'B1L14-II-11',\n",
    "        'B1L14-II-07': 'B1L14-II-12',\n",
    "        'B1L14-II-08': 'B1L14-II-13',\n",
    "        'B1L14-II-09': 'B1L14-II-14',\n",
    "        'B1L14-II-10': 'B1L14-II-15',\n",
    "        'B1L14-II-11': 'B1L14-II-16',\n",
    "        'B1L14-II-12': 'B1L14-II-17',\n",
    "        # skipped index in slides\n",
    "        'B2L08-I-19': 'B2L08-I-18',\n",
    "        'B2L08-I-20': 'B2L08-I-19',\n",
    "        'B2L08-I-21': 'B2L08-I-20',\n",
    "        'B2L08-I-22': 'B2L08-I-21',\n",
    "        # B6L01-II-22 missing in slides, shifted indexes\n",
    "        'B6L01-II-23': 'B6L01-II-24',\n",
    "        # extra dup word at B6L05-I-30\n",
    "        'B6L05-I-31': 'B6L05-I-30',\n",
    "        'B6L05-I-32': 'B6L05-I-31',\n",
    "        # misindexing, missing #45 maidan\n",
    "        'B6L08-I-45': 'B6L08-I-T-01',\n",
    "        # B6L10 misindexing in slides\n",
    "        'B6L10-I-0-05': 'B6L10-I-01',\n",
    "        'B6L10-I-0-06': 'B6L10-I-02',\n",
    "        'B6L10-I-0-07': 'B6L10-I-03',\n",
    "        'B6L10-I-0-08': 'B6L10-I-04',\n",
    "        'B6L10-I-0-09': 'B6L10-I-05',\n",
    "        'B6L10-I-35': 'B6L10-I-T-01',\n",
    "        'B6L10-I-36': 'B6L10-I-T-02',\n",
    "        'B6L10-I-37': 'B6L10-I-T-03',\n",
    "        'B6L10-I-38': 'B6L10-I-T-04',\n",
    "        'B6L10-I-39': 'B6L10-II-0-01',\n",
    "        'B6L10-I-40': 'B6L10-II-0-02',\n",
    "        'B6L10-I-41': 'B6L10-II-0-03',\n",
    "        'B6L10-I-42': 'B6L10-II-0-04',\n",
    "        'B6L10-I-43': 'B6L10-II-0-05',\n",
    "        'B6L10-I-44': 'B6L10-II-01',\n",
    "        'B6L10-I-45': 'B6L10-II-02',\n",
    "        'B6L10-I-46': 'B6L10-II-03',\n",
    "        'B6L10-I-47': 'B6L10-II-04',\n",
    "        'B6L10-I-48': 'B6L10-II-05',\n",
    "        'B6L10-I-49': 'B6L10-II-06',\n",
    "        'B6L10-I-T-01': 'B6L10-II-07',\n",
    "        'B6L10-I-T-02': 'B6L10-II-08',\n",
    "        'B6L10-II-T-01': 'B6L10-II-54',\n",
    "        'B6L10-II-T-02': 'B6L10-II-55',\n",
    "    }\n",
    "    for i in range(1, 21):\n",
    "        term_id_mp['B6L10-I-%.2d' % i] = 'B6L10-I-%.2d' % (5+i)\n",
    "    for i in range(21, 35):\n",
    "        term_id_mp['B6L10-I-%.2d' % i] = 'B6L10-I-%.2d' % (4+i)\n",
    "    for i in range(1, 46):\n",
    "        term_id_mp['B6L10-II-%.2d' % i] = 'B6L10-II-%.2d' % (8+i)\n",
    "    term_id = term_id_mp.get(term_id, term_id)\n",
    "\n",
    "    term_id = {\n",
    "        ('B5L07-II-39', '不勞而獲'): 'B5L07-II-38',\n",
    "        ('B5L07-II-39', '不劳而获'): 'B5L07-II-38',\n",
    "        ('B5L07-II-41', '拉'): 'B5L07-II-40',\n",
    "        ('B6L10-II-T-03', '連接'): 'B6L10-II-56',\n",
    "        ('B6L10-II-T-03', '動手'): 'B6L10-II-57',\n",
    "        ('B6L10-II-T-03', '人生苦短'): 'B6L10-II-58',\n",
    "        ('B6L10-II-T-03', '攻無不克'): 'B6L10-II-59',\n",
    "        ('B6L10-II-T-03', '事無不成'): 'B6L10-II-60',\n",
    "        ('B6L10-II-T-03', '失讀症'): 'B6L10-II-T-01',\n",
    "        ('B6L10-II-T-03', '李光耀'): 'B6L10-II-T-02',\n",
    "        ('B6L10-II-T-03', '妥瑞氏症'): 'B6L10-II-T-03',\n",
    "        ('B6L10-II-T-03', '鏡像神經元'): 'B6L10-II-T-04',\n",
    "        ('B6L10-II-T-03', '连接'): 'B6L10-II-56',\n",
    "        ('B6L10-II-T-03', '动手'): 'B6L10-II-57',\n",
    "        ('B6L10-II-T-03', '人生苦短'): 'B6L10-II-58',\n",
    "        ('B6L10-II-T-03', '攻无不克'): 'B6L10-II-59',\n",
    "        ('B6L10-II-T-03', '事无不成'): 'B6L10-II-60',\n",
    "        ('B6L10-II-T-03', '失读症'): 'B6L10-II-T-01',\n",
    "        ('B6L10-II-T-03', '李光耀'): 'B6L10-II-T-02',\n",
    "        ('B6L10-II-T-03', '妥瑞氏症'): 'B6L10-II-T-03',\n",
    "        ('B6L10-II-T-03', '镜像神经元'): 'B6L10-II-T-04',\n",
    "    }.get((term_id, para[0]), term_id)\n",
    "\n",
    "    # Fix some suboptimal text extraction cases\n",
    "    para = {\n",
    "        ('B2L01-I-18', '師大'): ['師大（師範大學）', 'Shīdà (Shīfàn Dàxué)', 'NTNU (National Taiwan Normal University)'],\n",
    "        ('B2L01-I-18', '师大'): ['师大（师范大学）', 'Shīdà (Shīfàn Dàxué)', 'NTNU (National Taiwan Normal University)'],\n",
    "        ('B2L14-II-24', '企業管理系'): ['企業管理系（企管系）', 'qìyè guǎnlǐ xì', 'department of business management'],\n",
    "        ('B2L14-II-24', '企业管理系'): ['企业管理系（企管系）', 'qìyè guǎnlǐ xì', 'department of business management'],\n",
    "        ('B3L08-II-46', 'EMBA（高級管理人員 工商管理碩士）'): ['EMBA（高級管理人員 工商管理碩士）', 'EMBA (gāojí guǎnlǐ rényuán gōngshāng guǎnlǐ shuòshì)', 'EMBA (Executive Master of Business Administration)'],\n",
    "        ('B3L08-II-46', 'EMBA（高级管理人员 工商管理硕士）'): ['EMBA（高级管理人员 工商管理硕士）', 'EMBA (gāojí guǎnlǐ rényuán gōngshāng guǎnlǐ shuòshì)', 'EMBA (Executive Master of Business Administration)'],\n",
    "    }.get((term_id, para[0]), para)\n",
    "\n",
    "    hanzi, para = extract_hanzi(term_id, para)\n",
    "\n",
    "    para_p = [(sum(c in 'āáǎàēéěèīíǐìōóǒòūúǔùüǖǘǚǜ' for c in w), -len(w), w) for w in para]\n",
    "    para_p.sort()\n",
    "    pinyin = para_p[-1][-1]\n",
    "    i = para.index(pinyin)\n",
    "    para = para[:i] + para[(i+1):]\n",
    "    pinyin = postprocess_chars(pinyin, False)\n",
    "\n",
    "    meaning = ' '.join(para).strip()\n",
    "    meaning = re.sub(r'\\s+', ' ', meaning)\n",
    "    meaning = meaning.replace('（', ' (')\n",
    "    meaning = meaning.replace('）', ')')\n",
    "    meaning = meaning.replace('  (', ' (')\n",
    "    meaning = re.sub(r' [)]', ')', meaning)\n",
    "    meaning = re.sub(r'^[．]', '', meaning)\n",
    "    meaning = postprocess_chars(meaning, False)\n",
    "\n",
    "    return {'id': term_id, 'hanzi': hanzi, 'pinyin': pinyin, 'meaning': meaning}\n",
    "\n",
    "\n",
    "parsed_terms = {}\n",
    "\n",
    "for cset in ['trad', 'simp']:\n",
    "    print('%s' % cset)\n",
    "    files = pptx_glob(cset)\n",
    "    assert len(files) == 15+15+12+12+10+10\n",
    "    parsed_terms[cset] = {}\n",
    "    term_ids_list = []\n",
    "\n",
    "    for filepath in files:\n",
    "        term_id_prefix, term_id_next = '', 1\n",
    "        term_aux = 0\n",
    "        fix_l6_tech = False\n",
    "        book_lesson = os.path.basename(filepath).replace('.pptx', '').replace('-', '')\n",
    "\n",
    "        for para in extract_para(filepath)[1:]:\n",
    "            parsed = parse_para(para, book_lesson, cset == 'simp')\n",
    "            term_id = parsed['id']\n",
    "    \n",
    "            if term_id == 'B5L01-II-11' and 'nìmíng' in para and cset == 'simp':\n",
    "                continue  # dup\n",
    "            if term_id == 'B6L05-I-30' and parsed['pinyin'] == 'yìtǔ wéikuài':\n",
    "                continue  # dup from B6L04\n",
    "            if term_id == 'B6L10-I-25' and parsed['pinyin'] == 'lǚchéng':\n",
    "                continue  # dup from B6L09\n",
    "            if term_id == 'B6L08-I-T-01' and parsed['pinyin'] == 'Niújīn Dàxué':\n",
    "                continue  # dup\n",
    "\n",
    "            # tech terms for second vocab in slides still have 'I-' ids leading to id dups\n",
    "            term_id = parsed['id']\n",
    "            if re.match('^B6L..-I-T-[0-9]*$', term_id):\n",
    "                if term_id.endswith('-01') and term_id in parsed_terms[cset] and book_lesson != 'B6L08':\n",
    "                    fix_l6_tech = True\n",
    "                if fix_l6_tech:\n",
    "                    term_id = term_id.replace('-I-T-', '-II-T-')\n",
    "                    parsed['id'] = term_id\n",
    "            else:\n",
    "                fix_l6_tech = False\n",
    "\n",
    "            # sequential ids check\n",
    "            m = re.match('^((B.L..)(-I-|-II-)(|[0T]-))[0-9][0-9]$', term_id)\n",
    "            assert m, (term_id, parsed)\n",
    "            if term_id_prefix != m[1]:\n",
    "                term_id_prefix = m[1]\n",
    "                term_id_next = 1\n",
    "            # missing terms\n",
    "            if term_id not in ['B6L01-II-24']:\n",
    "                assert term_id.endswith('-' + '%.2d' % term_id_next), (term_id_next, parsed)\n",
    "                term_id_next += 1\n",
    "            else:\n",
    "                term_id_next = int(term_id.split('-')[-1]) + 1\n",
    "\n",
    "            assert term_id not in parsed_terms[cset]  # uniq id check\n",
    "            parsed_terms[cset][term_id] = parsed\n",
    "\n",
    "            if term_ids_list:\n",
    "                assert term_id > term_ids_list[-1], term_id\n",
    "\n",
    "            term_ids_list.append(term_id)\n",
    "\n",
    "assert set(parsed_terms['simp'].keys()) == set(parsed_terms['trad'].keys())\n",
    "assert term_ids_list == list(sorted(term_ids_list))\n",
    "term_ids = term_ids_list\n",
    "\n",
    "for b in range(1, 7):\n",
    "    print('B%d: %d' % (b, len([s for s in term_ids if s.startswith('B%d' % b)])))\n",
    "print('Total: %d' % len(term_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa1e8b3-f7b8-43fa-96c3-099eaee3d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract POS / meaning proper\n",
    "\n",
    "POS_MAP = {\n",
    "  \"(Adv)\": \"Adv\",\n",
    "  \"(Adv/N)\": \"Adv/N\",\n",
    "  \"(Conj)\": \"Conj\",\n",
    "  \"(Det)\": \"Det\",\n",
    "  \"(Id)\": \"Id\",\n",
    "  \"(M)\": \"M\",\n",
    "  \"(N)\": \"N\",\n",
    "  \"(N/V)\": \"N/V\",\n",
    "  \"(N/Vst)\": \"N/Vst\",\n",
    "  \"(Ph)\": \"Ph\",\n",
    "  \"(Prep)\": \"Prep\",\n",
    "  \"(Ptc)\": \"Ptc\",\n",
    "  \"(V)\": \"V\",\n",
    "  \"(V-sep)\": \"V-sep\",\n",
    "  \"(V/ N)\": \"V/N\",\n",
    "  \"(V/N)\": \"V/N\",\n",
    "  \"(Vaux)\": \"Vaux\",\n",
    "  \"(Vi)\": \"Vi\",\n",
    "  \"(Vi, N)\": \"Vi/N\",\n",
    "  \"(Vi, V)\": \"Vi/V\",\n",
    "  \"(Vi/N)\": \"Vi/N\",\n",
    "  \"(Vp)\": \"Vp\",\n",
    "  \"(Vp-sep)\": \"Vp-sep\",\n",
    "  \"(Vpt)\": \"Vpt\",\n",
    "  \"(Vs)\": \"Vs\",\n",
    "  \"(Vs-attr)\": \"Vs-attr\",\n",
    "  \"(Vs-pred)\": \"Vs-pred\",\n",
    "  \"(Vs-sep)\": \"Vs-sep\",\n",
    "  \"(Vs/N)\": \"Vs/N\",\n",
    "  \"(Vst)\": \"Vst\",\n",
    "  \"(Vst/N)\": \"Vst/N\",\n",
    "  \"( N)\": \"N\",\n",
    "  \" ( N)\": \"N\",\n",
    "  \"(N/Vi)\": \"N/Vi\",\n",
    "  \"Adv \": \"Adv\",\n",
    "  \"N \": \"N\",\n",
    "}\n",
    "\n",
    "for term_id in term_ids_list:\n",
    "    trad = parsed_terms['trad'][term_id]\n",
    "    simp = parsed_terms['simp'][term_id]\n",
    "    assert simp['pinyin'] == trad['pinyin']\n",
    "    assert simp['meaning'] == opencc_tw2s.convert(trad['meaning']), (trad, simp)\n",
    "\n",
    "    meaning = trad['meaning'].strip()\n",
    "    pos = ''\n",
    "    pos_pref = ''\n",
    "    for pref in POS_MAP.keys():\n",
    "        if meaning.startswith(pref):\n",
    "            pos = POS_MAP[pref]\n",
    "            pos_pref = pref\n",
    "            break\n",
    "\n",
    "    trad['pos'] = pos\n",
    "    trad['pos_pref'] = pos_pref\n",
    "\n",
    "    meaning = trad['meaning'].strip()\n",
    "    if trad['pos_pref']:\n",
    "        assert meaning.startswith(trad['pos_pref']), trad\n",
    "        meaning = meaning[len(trad['pos_pref']):].strip()\n",
    "    trad['meaning_nopos'] = meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d683ed3f-da84-4206-8099-a00211d68744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927\n",
      "-rw-r--r-- 1 jovyan users 300501 Oct 20 14:30 data/slides.tsv\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "dd_t2s = {}\n",
    "\n",
    "for term_id in term_ids:\n",
    "    trad = parsed_terms['trad'][term_id]\n",
    "    simp = parsed_terms['simp'][term_id]\n",
    "    assert simp['pinyin'] == trad['pinyin']\n",
    "    assert simp['meaning'] == opencc_tw2s.convert(trad['meaning'])\n",
    "\n",
    "    if trad['hanzi'] not in dd_t2s:\n",
    "        dd_t2s[trad['hanzi']] = simp['hanzi']\n",
    "    assert dd_t2s[trad['hanzi']] == simp['hanzi']\n",
    "\n",
    "    rows.append({\n",
    "        'ID': trad['id'],\n",
    "        'Traditional': trad['hanzi'],\n",
    "        'Simplified': simp['hanzi'],\n",
    "        'Pinyin': trad['pinyin'],\n",
    "        'POS': trad['pos'],\n",
    "        'Meaning': trad['meaning_nopos'],\n",
    "        'Tags': '',\n",
    "    })\n",
    "\n",
    "slides_df = pd.DataFrame(rows)\n",
    "slides_df.to_csv('data/slides.tsv', sep='\\t', index=False)\n",
    "#slides_df[['ID', 'POS', 'Traditional']].to_csv('gen/term-ids-slides.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(len(slides_df))\n",
    "\n",
    "!ls -l data/slides.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
